{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22230,"status":"ok","timestamp":1746880477019,"user":{"displayName":"Hajar Chawki","userId":"18130003551262270235"},"user_tz":-60},"id":"yXW5-MGfzO6c","outputId":"f4c9c5a6-97dc-4fb5-a2c9-04f48f74d24b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70081,"status":"ok","timestamp":1746880548256,"user":{"displayName":"Hajar Chawki","userId":"18130003551262270235"},"user_tz":-60},"id":"wbHb3Oe1zm_T","outputId":"310f12e1-a291-44bb-9cdc-858d0bd0d1cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Collecting vit-pytorch\n","  Downloading vit_pytorch-1.10.1-py3-none-any.whl.metadata (69 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: einops>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from vit-pytorch) (0.8.1)\n","Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from vit-pytorch) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from vit-pytorch) (0.21.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->vit-pytorch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->vit-pytorch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->vit-pytorch) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->vit-pytorch) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->vit-pytorch) (3.0.2)\n","Downloading vit_pytorch-1.10.1-py3-none-any.whl (140 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.8/140.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: vit-pytorch\n","Successfully installed vit-pytorch-1.10.1\n"]}],"source":["!pip install torch\n","!pip install vit-pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":703},"executionInfo":{"elapsed":950406,"status":"error","timestamp":1746322578564,"user":{"displayName":"Hajar Chawki","userId":"18130003551262270235"},"user_tz":-60},"id":"AUoZX2R4zI5j","outputId":"7d11ee02-717b-4c47-b48a-aba6253ff116"},"outputs":[{"name":"stdout","output_type":"stream","text":["Détection de 4 classes d'actions\n","Using device: cuda\n","Epoch 1/30\n","Train Loss: 1.5736 | Acc: 29.52%\n","Val Loss: 1.5767 | Acc: 30.51%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_1.pth\n","Epoch 2/30\n","Train Loss: 1.2246 | Acc: 45.37%\n","Val Loss: 1.2173 | Acc: 52.54%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_2.pth\n","Epoch 3/30\n","Train Loss: 0.8619 | Acc: 70.48%\n","Val Loss: 0.7763 | Acc: 71.19%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_3.pth\n","Epoch 4/30\n","Train Loss: 0.4248 | Acc: 83.70%\n","Val Loss: 0.3932 | Acc: 86.44%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_4.pth\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-39d9d40f10c2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# Lancement de l'entraînement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_video_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-39d9d40f10c2>\u001b[0m in \u001b[0;36mtrain_video_model\u001b[0;34m(train_dir, val_dir, num_classes, num_epochs, save_path)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import numpy as np\n","import os\n","import cv2\n","from vit_pytorch import ViT\n","from einops import rearrange\n","\n","# 1. Modèle ViT adapté à la vidéo\n","class VideoViT(nn.Module):\n","    def __init__(self, num_classes, frames=32, image_size=224, dim=1024, depth=3, heads=8):\n","        super().__init__()\n","        self.vit = ViT(\n","            image_size=image_size,\n","            patch_size=16,\n","            num_classes=num_classes,\n","            dim=dim,\n","            depth=depth,\n","            heads=heads,\n","            mlp_dim=2048,\n","            channels=3,\n","            dim_head=64,\n","            dropout=0.1,\n","            emb_dropout=0.1\n","        )\n","        self.frames = frames\n","\n","    def forward(self, x):\n","        b, t, c, h, w = x.shape\n","        x = rearrange(x, 'b t c h w -> (b t) c h w')\n","        x = self.vit(x)\n","        x = rearrange(x, '(b t) d -> b t d', b=b, t=t)\n","        x = x.mean(dim=1)\n","        return x\n","\n","# 2. Dataset pour charger les vidéos MP4\n","class VideoDataset(Dataset):\n","    def __init__(self, data_dir, transform=None, num_frames=32, img_size=224):\n","        self.data_dir = data_dir\n","        self.transform = transform or self.default_transform(img_size)\n","        self.num_frames = num_frames\n","        self.classes = sorted(os.listdir(data_dir))\n","        self.video_paths = []\n","\n","        for label, class_name in enumerate(self.classes):\n","            class_dir = os.path.join(data_dir, class_name)\n","            for file in os.listdir(class_dir):\n","                if file.endswith('.mp4'):\n","                    self.video_paths.append((os.path.join(class_dir, file), label))\n","\n","    @staticmethod\n","    def default_transform(img_size):\n","        return transforms.Compose([\n","            transforms.ToPILImage(),\n","            transforms.Resize((img_size, img_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","\n","    def __len__(self):\n","        return len(self.video_paths)\n","\n","    def __getitem__(self, idx):\n","        video_path, label = self.video_paths[idx]\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        indices = np.linspace(0, total_frames-1, num=self.num_frames, dtype=int)\n","\n","        for i in indices:\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","            ret, frame = cap.read()\n","            if ret:\n","                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                frame = self.transform(frame)\n","                frames.append(frame)\n","\n","        cap.release()\n","\n","        while len(frames) < self.num_frames:\n","            frames.append(torch.zeros_like(frames[0]))\n","\n","        video_tensor = torch.stack(frames)\n","        return video_tensor, torch.tensor(label, dtype=torch.long)\n","\n","# 3. Fonction d'entraînement avec sauvegarde à chaque epoch\n","def train_video_model(train_dir, val_dir, num_classes, num_epochs=30, save_path=\"models/\"):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")\n","\n","    os.makedirs(save_path, exist_ok=True)\n","\n","    batch_size = 6\n","    num_frames = 32\n","    img_size = 224\n","\n","    train_dataset = VideoDataset(train_dir, num_frames=num_frames, img_size=img_size)\n","    val_dataset = VideoDataset(val_dir, num_frames=num_frames, img_size=img_size)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n","\n","    model = VideoViT(num_classes=num_classes, frames=num_frames, image_size=img_size).to(device)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for videos, labels in train_loader:\n","            videos, labels = videos.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(videos)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            correct += predicted.eq(labels).sum().item()\n","            total += labels.size(0)\n","\n","        scheduler.step()\n","\n","        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}')\n","        print(f'Train Loss: {train_loss/len(train_loader):.4f} | Acc: {100.*correct/total:.2f}%')\n","        print(f'Val Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%')\n","        print('-'*50)\n","\n","        # 🔥 Sauvegarder modèle à chaque epoch\n","        model_filename = f\"{save_path}/video_vit_epoch_{epoch+1}.pth\"\n","        torch.save(model.state_dict(), model_filename)\n","        print(f\"✅ Modèle sauvegardé : {model_filename}\")\n","\n","    return model\n","\n","# 4. Fonction d'évaluation\n","def evaluate(model, loader, criterion, device):\n","    model.eval()\n","    loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for videos, labels in loader:\n","            videos, labels = videos.to(device), labels.to(device)\n","            outputs = model(videos)\n","            loss += criterion(outputs, labels).item()\n","            _, predicted = outputs.max(1)\n","            correct += predicted.eq(labels).sum().item()\n","            total += labels.size(0)\n","\n","    return loss/len(loader), 100.*correct/total\n","\n","# 5. Exécution\n","if __name__ == \"__main__\":\n","    TRAIN_DIR = \"/content/drive/MyDrive/videos/train\"\n","    VAL_DIR = \"/content/drive/MyDrive/videos/val\"\n","    NUM_CLASSES = len(os.listdir(TRAIN_DIR))\n","\n","    print(f\"Détection de {NUM_CLASSES} classes d'actions\")\n","\n","    # Lancement de l'entraînement\n","    model = train_video_model(TRAIN_DIR, VAL_DIR, NUM_CLASSES, num_epochs=30, save_path=\"/content/drive/MyDrive/models\")\n"]},{"cell_type":"markdown","metadata":{"id":"dP4dvkari5R6"},"source":["# **reprendre l’entraînement depuis l’état enregistré**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"m-DO7D4Ui4-H","outputId":"8feeef5b-2735-4a52-e3a8-502dfd9eb05f"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔁 Reprise à partir de l’epoch 21\n","Using device: cuda\n","🔄 Modèle chargé depuis /content/drive/MyDrive/models/video_vit_epoch_4.pth (reprise à l’epoch 21)\n","Epoch 21/30\n","Train Loss: 0.2767 | Acc: 88.99%\n","Val Loss: 0.4006 | Acc: 89.83%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_21.pth\n","Epoch 22/30\n","Train Loss: 0.1854 | Acc: 92.07%\n","Val Loss: 0.1606 | Acc: 96.61%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_22.pth\n","Epoch 23/30\n","Train Loss: 0.1320 | Acc: 95.15%\n","Val Loss: 0.2628 | Acc: 89.83%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_23.pth\n","Epoch 24/30\n","Train Loss: 0.1269 | Acc: 95.15%\n","Val Loss: 0.1252 | Acc: 94.92%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_24.pth\n","Epoch 25/30\n","Train Loss: 0.0929 | Acc: 97.36%\n","Val Loss: 0.2430 | Acc: 93.22%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_25.pth\n","Epoch 26/30\n","Train Loss: 0.0948 | Acc: 96.04%\n","Val Loss: 0.2299 | Acc: 93.22%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_26.pth\n","Epoch 27/30\n","Train Loss: 0.0647 | Acc: 98.24%\n","Val Loss: 0.2729 | Acc: 89.83%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_27.pth\n","Epoch 28/30\n","Train Loss: 0.1820 | Acc: 93.39%\n","Val Loss: 0.2066 | Acc: 91.53%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_28.pth\n","Epoch 29/30\n","Train Loss: 0.0858 | Acc: 96.48%\n","Val Loss: 0.0857 | Acc: 96.61%\n","--------------------------------------------------\n","✅ Modèle sauvegardé : /content/drive/MyDrive/models/video_vit_epoch_29.pth\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import numpy as np\n","import os\n","import cv2\n","from vit_pytorch import ViT\n","from einops import rearrange\n","\n","# 1. Modèle ViT adapté à la vidéo\n","class VideoViT(nn.Module):\n","    def __init__(self, num_classes, frames=32, image_size=224, dim=1024, depth=3, heads=8):\n","        super().__init__()\n","        self.vit = ViT(\n","            image_size=image_size,\n","            patch_size=16,\n","            num_classes=num_classes,\n","            dim=dim,\n","            depth=depth,\n","            heads=heads,\n","            mlp_dim=2048,\n","            channels=3,\n","            dim_head=64,\n","            dropout=0.1,\n","            emb_dropout=0.1\n","        )\n","        self.frames = frames\n","\n","    def forward(self, x):\n","        b, t, c, h, w = x.shape\n","        x = rearrange(x, 'b t c h w -> (b t) c h w')\n","        x = self.vit(x)\n","        x = rearrange(x, '(b t) d -> b t d', b=b, t=t)\n","        x = x.mean(dim=1)\n","        return x\n","\n","# 2. Dataset pour charger les vidéos MP4\n","class VideoDataset(Dataset):\n","    def __init__(self, data_dir, transform=None, num_frames=32, img_size=224):\n","        self.data_dir = data_dir\n","        self.transform = transform or self.default_transform(img_size)\n","        self.num_frames = num_frames\n","        self.classes = sorted(os.listdir(data_dir))\n","        self.video_paths = []\n","\n","        for label, class_name in enumerate(self.classes):\n","            class_dir = os.path.join(data_dir, class_name)\n","            for file in os.listdir(class_dir):\n","                if file.endswith('.mp4'):\n","                    self.video_paths.append((os.path.join(class_dir, file), label))\n","\n","    @staticmethod\n","    def default_transform(img_size):\n","        return transforms.Compose([\n","            transforms.ToPILImage(),\n","            transforms.Resize((img_size, img_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","\n","    def __len__(self):\n","        return len(self.video_paths)\n","\n","    def __getitem__(self, idx):\n","        video_path, label = self.video_paths[idx]\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        indices = np.linspace(0, total_frames-1, num=self.num_frames, dtype=int)\n","\n","        for i in indices:\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","            ret, frame = cap.read()\n","            if ret:\n","                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                frame = self.transform(frame)\n","                frames.append(frame)\n","\n","        cap.release()\n","\n","        while len(frames) < self.num_frames:\n","            frames.append(torch.zeros_like(frames[0]))\n","\n","        video_tensor = torch.stack(frames)\n","        return video_tensor, torch.tensor(label, dtype=torch.long)\n","\n","# 3. Fonction d'entraînement avec reprise possible\n","def train_video_model(train_dir, val_dir, num_classes, num_epochs=30, save_path=\"models/\",\n","                      resume_path=None, resume_epoch=0):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")\n","\n","    os.makedirs(save_path, exist_ok=True)\n","\n","    batch_size = 6\n","    num_frames = 32\n","    img_size = 224\n","\n","    train_dataset = VideoDataset(train_dir, num_frames=num_frames, img_size=img_size)\n","    val_dataset = VideoDataset(val_dir, num_frames=num_frames, img_size=img_size)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n","\n","    model = VideoViT(num_classes=num_classes, frames=num_frames, image_size=img_size).to(device)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    if resume_path:\n","        model.load_state_dict(torch.load(resume_path, map_location=device))\n","        print(f\"🔄 Modèle chargé depuis {resume_path} (reprise à l’epoch {resume_epoch+1})\")\n","\n","    for epoch in range(resume_epoch, num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for videos, labels in train_loader:\n","            videos, labels = videos.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(videos)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            correct += predicted.eq(labels).sum().item()\n","            total += labels.size(0)\n","\n","        scheduler.step()\n","\n","        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}')\n","        print(f'Train Loss: {train_loss/len(train_loader):.4f} | Acc: {100.*correct/total:.2f}%')\n","        print(f'Val Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%')\n","        print('-'*50)\n","\n","        model_filename = f\"{save_path}/video_vit_epoch_{epoch+1}.pth\"\n","        torch.save(model.state_dict(), model_filename)\n","        print(f\"✅ Modèle sauvegardé : {model_filename}\")\n","\n","    return model\n","\n","# 4. Fonction d'évaluation\n","def evaluate(model, loader, criterion, device):\n","    model.eval()\n","    loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for videos, labels in loader:\n","            videos, labels = videos.to(device), labels.to(device)\n","            outputs = model(videos)\n","            loss += criterion(outputs, labels).item()\n","            _, predicted = outputs.max(1)\n","            correct += predicted.eq(labels).sum().item()\n","            total += labels.size(0)\n","\n","    return loss/len(loader), 100.*correct/total\n","\n","# 5. Exécution principale avec REPRISE\n","if __name__ == \"__main__\":\n","    TRAIN_DIR = \"/content/drive/MyDrive/videos/train\"\n","    VAL_DIR = \"/content/drive/MyDrive/videos/val\"\n","    SAVE_PATH = \"/content/drive/MyDrive/models\"\n","    NUM_CLASSES = len(os.listdir(TRAIN_DIR))\n","\n","    resume_model = os.path.join(SAVE_PATH, \"video_vit_epoch_4.pth\")\n","    resume_epoch = 20  # donc on va commencer à epoch 5\n","\n","    print(f\"🔁 Reprise à partir de l’epoch {resume_epoch + 1}\")\n","\n","    model = train_video_model(\n","        train_dir=TRAIN_DIR,\n","        val_dir=VAL_DIR,\n","        num_classes=NUM_CLASSES,\n","        num_epochs=30,  # total epochs à atteindre\n","        save_path=SAVE_PATH,\n","        resume_path=resume_model,\n","        resume_epoch=resume_epoch\n","    )\n"]},{"cell_type":"markdown","metadata":{"id":"TaW4-LHIixhz"},"source":["# ** pour tester sur un video**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22044,"status":"ok","timestamp":1746880586529,"user":{"displayName":"Hajar Chawki","userId":"18130003551262270235"},"user_tz":-60},"id":"MgUXi88i2eEF","outputId":"fa25db61-67c0-4020-c6e7-58192093ae32"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vidéo testée : Meet and Split (52).mp4\n","Classe prédite : walking\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","import numpy as np\n","import os\n","import cv2\n","from vit_pytorch import ViT\n","from einops import rearrange\n","\n","# === 1. Définir le modèle identique à celui entraîné ===\n","class VideoViT(nn.Module):\n","    def __init__(self, num_classes, frames=32, image_size=224, dim=1024, depth=3, heads=8):\n","        super().__init__()\n","        self.vit = ViT(\n","            image_size=image_size,\n","            patch_size=16,\n","            num_classes=num_classes,\n","            dim=dim,\n","            depth=depth,\n","            heads=heads,\n","            mlp_dim=2048,\n","            channels=3,\n","            dim_head=64,\n","            dropout=0.1,\n","            emb_dropout=0.1\n","        )\n","\n","    def forward(self, x):\n","        b, t, c, h, w = x.shape\n","        x = rearrange(x, 'b t c h w -> (b t) c h w')\n","        x = self.vit(x)\n","        x = rearrange(x, '(b t) d -> b t d', b=b, t=t)\n","        return x.mean(dim=1)  # Pooling temporel\n","\n","# === 2. Prétraitement de la vidéo ===\n","def preprocess_video(video_path, num_frames=32, img_size=224):\n","    transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((img_size, img_size)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ])\n","\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n","\n","    for i in indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","        ret, frame = cap.read()\n","        if ret:\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frame = transform(frame)\n","            frames.append(frame)\n","\n","    cap.release()\n","\n","    while len(frames) < num_frames:\n","        frames.append(torch.zeros_like(frames[0]))\n","\n","    video_tensor = torch.stack(frames)  # (T, C, H, W)\n","    video_tensor = video_tensor.unsqueeze(0)  # (1, T, C, H, W)\n","    return video_tensor\n","\n","# === 3. Prédiction ===\n","def predict_video_class(model_path, video_path, class_names, num_classes):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Charger le modèle\n","    model = VideoViT(num_classes=num_classes).to(device)\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.eval()\n","\n","    # Charger et préparer la vidéo\n","    video_tensor = preprocess_video(video_path).to(device)\n","\n","    # Prédiction\n","    with torch.no_grad():\n","        output = model(video_tensor)\n","        predicted_idx = output.argmax(dim=1).item()\n","        predicted_class = class_names[predicted_idx]\n","\n","    print(f\"Vidéo testée : {os.path.basename(video_path)}\")\n","    print(f\"Classe prédite : {predicted_class}\")\n","\n","# === 4. Exemple d’utilisation ===\n","if __name__ == \"__main__\":\n","    model_path = \"/content/drive/MyDrive/models/video_vit_epoch_30.pth\"\n","    video_path = \"/content/drive/MyDrive/ccc.mp4\"  # chemin vers la vidéo à tester\n","    video_path1 = \"/content/drive/MyDrive/videos/val/Meet and Split/Meet and Split (52).mp4\"\n","    class_names = ['walking','Sitting','Standing Still','Meet and Split']  # <- à adapter selon tes classes\n","    num_classes = len(class_names)\n","\n","    predict_video_class(model_path, video_path1, class_names, num_classes)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyP/yC8NlXXI4DVdyVuF+lYi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}